{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymethylprocess.MethylationDataTypes import MethylationArray\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.offline as py\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybedtools import BedTool\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BedTool(hg19.1m.bed)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BedTool('hg19.genome').makewindows(g='hg19.genome',w=1000000).saveas('hg19.1m.bed')#.to_dataframe().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma=MethylationArray.from_pickle('train_val_test_sets/train_methyl_array.pkl')\n",
    "ma_v=MethylationArray.from_pickle('train_val_test_sets/val_methyl_array.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_last=False\n",
    "def get_final_modules(ma=ma,a='450kannotations.bed',b='lola_vignette_data/activeDHS_universe.bed', include_last=False):\n",
    "    allcpgs=ma.beta.columns.values\n",
    "    df=BedTool(a).to_dataframe()\n",
    "    df.iloc[:,0]=df.iloc[:,0].astype(str).map(lambda x: 'chr'+x.split('.')[0])\n",
    "    df=df.set_index('name').loc[list(ma.beta)].reset_index().iloc[:,[1,2,3,0]]\n",
    "    df_bed=pd.read_table(b,header=None)\n",
    "    df_bed['features']=np.arange(df_bed.shape[0])\n",
    "    df_bed=df_bed.iloc[:,[0,1,2,-1]]\n",
    "    b=BedTool.from_dataframe(df)\n",
    "    a=BedTool.from_dataframe(df_bed)#('lola_vignette_data/activeDHS_universe.bed')\n",
    "    c=a.intersect(b,wa=True,wb=True).sort()\n",
    "    d=c.groupby(g=[1,2,3,4],c=(8,8),o=('count','distinct'))\n",
    "    df2=d.to_dataframe()\n",
    "    df3=df2.loc[df2.iloc[:,-2]>25]\n",
    "    modules = [cpgs.split(',') for cpgs in df3.iloc[:,-1].values]\n",
    "    modulecpgs=np.array(list(set(list(reduce(lambda x,y:x+y,modules)))))\n",
    "    missing_cpgs=np.setdiff1d(allcpgs,modulecpgs).tolist()\n",
    "    final_modules = modules+([missing_cpgs] if include_last else [])\n",
    "    module_names=(df3.iloc[:,0]+'_'+df3.iloc[:,1].astype(str)+'_'+df3.iloc[:,2].astype(str)).tolist()\n",
    "    return final_modules,modulecpgs,module_names\n",
    "\n",
    "final_modules,modulecpgs,module_names=get_final_modules(b='hg19.1m.bed',include_last=include_last)\n",
    "ma.beta=ma.beta.loc[:,modulecpgs]\n",
    "ma_v.beta=ma_v.beta.loc[:,modulecpgs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7747\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(map(len,final_modules))).sum()\n",
    "#len(final_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def softmax(input_tensor, dim=1):\n",
    "    # transpose input\n",
    "    transposed_input = input_tensor.transpose(dim, len(input_tensor.size()) - 1)\n",
    "    # calculate softmax\n",
    "    softmaxed_output = F.softmax(transposed_input.contiguous().view(-1, transposed_input.size(-1)), dim=-1)\n",
    "    # un-transpose result\n",
    "    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input_tensor.size()) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "# https://github.com/higgsfield/Capsule-Network-Tutorial/blob/master/Capsule%20Network.ipynb\n",
    "\n",
    "class MLP(nn.Module): # add latent space extraction, and spits out csv line of SQL as text for UMAP\n",
    "    def __init__(self, n_input, hidden_topology, dropout_p, n_outputs=1, binary=False, softmax=False):\n",
    "        super(MLP,self).__init__()\n",
    "        self.hidden_topology=hidden_topology\n",
    "        self.topology = [n_input]+hidden_topology+[n_outputs]\n",
    "        layers = [nn.Linear(self.topology[i],self.topology[i+1]) for i in range(len(self.topology)-2)]\n",
    "        for layer in layers:\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "        self.layers = [nn.Sequential(layer,nn.ReLU(),nn.Dropout(p=dropout_p)) for layer in layers]\n",
    "        self.output_layer = nn.Linear(self.topology[-2],self.topology[-1])\n",
    "        torch.nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "        if binary:\n",
    "            output_transform = nn.Sigmoid()\n",
    "        elif softmax:\n",
    "            output_transform = nn.Softmax()\n",
    "        else:\n",
    "            output_transform = nn.Dropout(p=0.)\n",
    "        self.layers.append(nn.Sequential(self.output_layer,output_transform))\n",
    "        self.mlp = nn.Sequential(*self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        return self.mlp(x)\n",
    "        \n",
    "class MethylationDataset(Dataset):\n",
    "    def __init__(self, methyl_arr, outcome_col,binarizer=None, modules=[]):\n",
    "        if binarizer==None:\n",
    "            binarizer=LabelBinarizer()\n",
    "            binarizer.fit(methyl_arr.pheno[outcome_col].astype(str).values)\n",
    "        self.y=binarizer.transform(methyl_arr.pheno[outcome_col].astype(str).values)\n",
    "        self.y_unique=np.unique(np.argmax(self.y,1))\n",
    "        self.binarizer=binarizer\n",
    "        if not modules:\n",
    "            modules=[list(methyl_arr.beta)]\n",
    "        self.modules=modules\n",
    "        self.X=methyl_arr.beta\n",
    "        self.length=methyl_arr.beta.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        return tuple([torch.FloatTensor(self.X.iloc[i].values)]+[torch.FloatTensor(self.X.iloc[i].loc[module].values) for module in self.modules]+[torch.FloatTensor(self.y[i])])\n",
    "    \n",
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self,modules,hidden_topology,n_output):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "        self.capsules=nn.ModuleList([MLP(len(module),hidden_topology,0.,n_outputs=n_output) for module in modules])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(self.capsules)\n",
    "        u = [self.capsules[i](x[i]) for i in range(len(self.capsules))]\n",
    "        u = torch.stack(u, dim=1)\n",
    "        #print(u.size())\n",
    "        return self.squash(u)\n",
    "    \n",
    "    def squash(self, x):\n",
    "        squared_norm = (x ** 2).sum(-1, keepdim=True)\n",
    "        #print('prim_norm',squared_norm.size())\n",
    "        output_tensor = squared_norm *  x / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        #print('z_init',output_tensor.size())\n",
    "        return output_tensor\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return list(self.capsules[0].parameters())[0].data#self.state_dict()#[self.capsules[i].state_dict() for i in range(len(self.capsules))]\n",
    "        \n",
    "class CapsLayer(nn.Module):\n",
    "    def __init__(self, n_capsules, n_routes, n_input, n_output, routing_iterations=3):\n",
    "        super(CapsLayer, self).__init__()\n",
    "        self.n_capsules=n_capsules\n",
    "        self.num_routes = n_routes\n",
    "        self.W=nn.Parameter(torch.randn(1, n_routes, n_capsules, n_output, n_input))\n",
    "        self.routing_iterations=routing_iterations\n",
    "        \n",
    "    def forward(self,x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.stack([x] * self.n_capsules, dim=2).unsqueeze(4)\n",
    "        \n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        #print('affine',W.size(),x.size())\n",
    "        u_hat = torch.matmul(W, x)\n",
    "        #print('affine_trans',u_hat.size())\n",
    "\n",
    "        b_ij = Variable(torch.zeros(1, self.num_routes, self.n_capsules, 1))\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            b_ij=b_ij.cuda()\n",
    "            \n",
    "\n",
    "        for iteration in range(self.routing_iterations):\n",
    "            c_ij = softmax(b_ij)\n",
    "            #print(c_ij)\n",
    "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n",
    "            #print('coeff',c_ij.size())#[0,:,0,:])#.size())\n",
    "\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "            v_j = self.squash(s_j)\n",
    "            #print('z',v_j.size())\n",
    "            \n",
    "            if iteration < self.routing_iterations - 1:\n",
    "                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n",
    "                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n",
    "\n",
    "        return v_j.squeeze(1)\n",
    "        \n",
    "        \n",
    "    def squash(self, x):\n",
    "        #print(x.size())\n",
    "        squared_norm = (x ** 2).sum(-1, keepdim=True)\n",
    "        #print('norm',squared_norm.size())\n",
    "        output_tensor = squared_norm *  x / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_input, n_output, hidden_topology):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder=MLP(n_input,hidden_topology, 0., n_outputs=n_output, binary=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, primary_caps, caps_hidden_layers, caps_output_layer, decoder, lr_balance=0.5, gamma=0.005):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.primary_caps=primary_caps\n",
    "        self.caps_hidden_layers=caps_hidden_layers\n",
    "        self.caps_output_layer=caps_output_layer\n",
    "        self.decoder=decoder\n",
    "        self.recon_loss_fn = nn.BCELoss()\n",
    "        self.lr_balance=lr_balance\n",
    "        self.gamma=gamma\n",
    "        \n",
    "    def forward(self, x_orig, modules_input):\n",
    "        x=self.primary_caps(modules_input)\n",
    "        primary_caps_out=x#.view(x.size(0),x.size(1)*x.size(2))\n",
    "        #print(x.size())\n",
    "        for layer in self.caps_hidden_layers:\n",
    "            x=layer(x)\n",
    "        \n",
    "        y_pred=self.caps_output_layer(x)#.squeeze(-1)\n",
    "        #print(y_pred.shape)\n",
    "        \n",
    "        classes = torch.sqrt((y_pred ** 2).sum(2))\n",
    "        classes = F.softmax(classes)\n",
    "        \n",
    "        max_length_indices = classes.argmax(dim=1)\n",
    "        masked = torch.sparse.torch.eye(8)\n",
    "        masked = masked.index_select(dim=0, index=max_length_indices.squeeze(1).data)\n",
    "        \n",
    "        embedding = (y_pred * masked[:, :, None, None]).view(y_pred.size(0), -1)\n",
    "        \n",
    "        #print(y_pred.size())\n",
    "        x_hat=self.decoder(embedding)#.reshape(y_pred.size(0),-1))\n",
    "        return x_orig, x_hat, y_pred, embedding, primary_caps_out\n",
    "    \n",
    "    def recon_loss(self, x_orig, x_hat):\n",
    "        return self.recon_loss_fn(x_hat, x_orig)\n",
    "    \n",
    "    def margin_loss(self,x, labels):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n",
    "        \n",
    "        #print(v_c)\n",
    "\n",
    "        left = (F.relu(0.9 - v_c)**2).view(batch_size, -1)\n",
    "        right = (F.relu(v_c - 0.1)**2).view(batch_size, -1)\n",
    "        #print(left)\n",
    "        #print(right)\n",
    "        #print(labels)\n",
    "\n",
    "        loss = labels * left + self.lr_balance * (1.0 - labels) * right\n",
    "        #print(loss.shape)\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "        return loss\n",
    "    \n",
    "    def calculate_loss(self, x_orig, x_hat, y_pred, y_true):\n",
    "        margin_loss = self.margin_loss(y_pred, y_true)\n",
    "        recon_loss = self.gamma*self.recon_loss(x_orig,x_hat)\n",
    "        loss = margin_loss + recon_loss\n",
    "        return loss, margin_loss, recon_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.pheno['Age_binned'],bins=pd.cut(ma.pheno['Age'],bins=8,retbins=True)\n",
    "ma_v.pheno['Age_binned'],bins=pd.cut(ma_v.pheno['Age'],bins=bins,retbins=True,)\n",
    "\n",
    "dataset=MethylationDataset(ma,'Age_binned',modules=final_modules)\n",
    "dataset_v=MethylationDataset(ma_v,'Age_binned',modules=final_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=DataLoader(dataset,batch_size=16,shuffle=True,num_workers=6, drop_last=True)\n",
    "dataloader_v=DataLoader(dataset_v,batch_size=16,shuffle=False,num_workers=6, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 0: Train Loss 0.5725887553258375, Train R2: -0.7333907141798888, Train MAE: 2.125\n",
      "Epoch 0: Val Loss 0.5783395767211914, Margin Loss 0.5714452713727951, Recon Loss 0.006894311518408358, Val R2: -2.8299663299663296, Val MAE: 3.32\n",
      "1\n",
      "Epoch 1: Train Loss 0.5145805315537886, Train R2: 0.300931968421721, Train MAE: 1.1647727272727273\n",
      "Epoch 1: Val Loss 0.5092523768544197, Margin Loss 0.5024370849132538, Recon Loss 0.006815279251895845, Val R2: 0.058291245791245894, Val MAE: 1.38\n",
      "2\n",
      "Epoch 2: Train Loss 0.47692854837937787, Train R2: 0.5827942120598548, Train MAE: 0.8068181818181818\n",
      "Epoch 2: Val Loss 0.4920669347047806, Margin Loss 0.4853948801755905, Recon Loss 0.006672052899375558, Val R2: -0.020622895622895543, Val MAE: 1.64\n",
      "3\n",
      "Epoch 3: Train Loss 0.45093053579330444, Train R2: 0.625011464410941, Train MAE: 0.7159090909090909\n",
      "Epoch 3: Val Loss 0.4829041138291359, Margin Loss 0.47646401077508926, Recon Loss 0.006440094206482172, Val R2: -0.38362794612794593, Val MAE: 1.7\n",
      "4\n",
      "Epoch 4: Train Loss 0.4391618885777213, Train R2: 0.5716190570257438, Train MAE: 0.7443181818181818\n"
     ]
    }
   ],
   "source": [
    "primary_caps_out_len=40\n",
    "caps_out_len=20\n",
    "n_inputs=list(map(len,final_modules))\n",
    "n_epochs=500\n",
    "n_primary=len(final_modules)\n",
    "hidden_topology=[30,100,100]\n",
    "gamma=1e-2\n",
    "decoder_top=[100,300]\n",
    "lr=1e-3\n",
    "routing_iterations=3\n",
    "\n",
    "primary_caps = PrimaryCaps(modules=final_modules,hidden_topology=hidden_topology,n_output=primary_caps_out_len)\n",
    "hidden_caps = []\n",
    "n_out_caps=len(dataset.y_unique)\n",
    "output_caps = CapsLayer(n_out_caps,n_primary,primary_caps_out_len,caps_out_len,routing_iterations=routing_iterations)\n",
    "decoder = Decoder(n_out_caps*caps_out_len,len(list(ma.beta)),decoder_top)\n",
    "capsnet = CapsNet(primary_caps, hidden_caps, output_caps, decoder, gamma=gamma)\n",
    "\n",
    "for d in ['figures/embeddings'+x for x in ['','1','2']]:\n",
    "    os.makedirs(d,exist_ok=True)\n",
    "\n",
    "optimizer = Adam(capsnet.parameters(),lr)\n",
    "scheduler=CosineAnnealingLR(optimizer, T_max=10, eta_min=0, last_epoch=-1)\n",
    "for epoch in range(n_epochs):\n",
    "    print(epoch)\n",
    "    capsnet.train(True)\n",
    "    running_loss=0.\n",
    "    Y={'true':[],'pred':[]}\n",
    "    for i,batch in enumerate(dataloader):\n",
    "        x_orig=batch[0]\n",
    "        #print(x_orig)\n",
    "        y_true=batch[-1]\n",
    "        module_x = batch[1:-1]\n",
    "        x_orig, x_hat, y_pred, embedding, primary_caps_out=capsnet(x_orig,module_x)\n",
    "        loss,margin_loss,recon_loss=capsnet.calculate_loss(x_orig, x_hat, y_pred, y_true)\n",
    "        Y['true'].extend(y_true.argmax(1).detach().cpu().numpy().tolist())\n",
    "        Y['pred'].extend(F.softmax(torch.sqrt((y_pred**2).sum(2))).argmax(1).detach().cpu().numpy().tolist())\n",
    "        train_loss=margin_loss.item()#print(loss)\n",
    "        running_loss+=train_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #print(capsnet.primary_caps.get_weights())\n",
    "    running_loss/=(i+1)\n",
    "    print('Epoch {}: Train Loss {}, Train R2: {}, Train MAE: {}'.format(epoch,running_loss,r2_score(Y['true'],Y['pred']), mean_absolute_error(Y['true'],Y['pred'])))\n",
    "    scheduler.step()\n",
    "    capsnet.train(False)\n",
    "    running_loss=np.zeros((3,))\n",
    "    Y={'true':[],'pred':[],'embeddings':[],'embeddings2':[],'embeddings3':[]}\n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(dataloader_v):\n",
    "            x_orig=batch[0]\n",
    "            y_true=batch[-1]\n",
    "            module_x = batch[1:-1]\n",
    "            x_orig, x_hat, y_pred, embedding, primary_caps_out=capsnet(x_orig,module_x)\n",
    "            #print(primary_caps_out.size())\n",
    "            Y['embeddings3'].append(torch.cat([primary_caps_out[i] for i in range(x_orig.size(0))],dim=0).detach().cpu().numpy())\n",
    "            primary_caps_out=primary_caps_out.view(primary_caps_out.size(0),primary_caps_out.size(1)*primary_caps_out.size(2))\n",
    "            Y['embeddings'].append(embedding.detach().cpu().numpy())\n",
    "            Y['embeddings2'].append(primary_caps_out.detach().cpu().numpy())\n",
    "            loss,margin_loss,recon_loss=capsnet.calculate_loss(x_orig, x_hat, y_pred, y_true)\n",
    "            val_loss=margin_loss.item()#print(loss)\n",
    "            running_loss+=np.array([loss,margin_loss,recon_loss])\n",
    "            Y['true'].extend(y_true.argmax(1).detach().cpu().numpy().tolist())\n",
    "            Y['pred'].extend((y_pred**2).sum(2).argmax(1).detach().cpu().numpy().tolist())\n",
    "        running_loss/=(i+1)\n",
    "    Y['embeddings']=pd.DataFrame(PCA(n_components=2).fit_transform(np.vstack(Y['embeddings'])),columns=['x','y'])\n",
    "    Y['embeddings2']=pd.DataFrame(PCA(n_components=2).fit_transform(np.vstack(Y['embeddings2'])),columns=['x','y'])\n",
    "    #print(list(map(lambda x: x.shape,Y['embeddings3'])))\n",
    "    Y['embeddings3']=pd.DataFrame(PCA(n_components=3).fit_transform(np.vstack(Y['embeddings3'])),columns=['x','y','z'])\n",
    "    Y['embeddings']['color']=Y['true']\n",
    "    Y['embeddings2']['color']=Y['true']\n",
    "    Y['embeddings3']['color']=module_names*ma_v.beta.shape[0]#Y['true']\n",
    "    Y['embeddings3']['name']=list(reduce(lambda x,y:x+y,[[i]*n_primary for i in Y['true']]))\n",
    "    fig = px.scatter_3d(Y['embeddings3'], x=\"x\", y=\"y\", z=\"z\", color=\"color\", symbol='name', text='name')\n",
    "    py.plot(fig, filename='figures/embeddings3/embeddings3.{}.pos.html'.format(epoch),auto_open=False)\n",
    "    #Y['embeddings3']['color']=list(reduce(lambda x,y:x+y,[[i]*n_primary for i in Y['true']]))\n",
    "    fig = px.scatter_3d(Y['embeddings3'], x=\"x\", y=\"y\", z='z', color=\"name\", text='color')\n",
    "    py.plot(fig, filename='figures/embeddings3/embeddings3.{}.true.html'.format(epoch),auto_open=False)\n",
    "    fig = px.scatter(Y['embeddings'], x=\"x\", y=\"y\", color=\"color\")\n",
    "    py.plot(fig, filename='figures/embeddings/embeddings.{}.true.html'.format(epoch),auto_open=False)\n",
    "    fig = px.scatter(Y['embeddings2'], x=\"x\", y=\"y\", color=\"color\")\n",
    "    py.plot(fig, filename='figures/embeddings2/embeddings2.{}.true.html'.format(epoch),auto_open=False)\n",
    "    Y['embeddings'].loc[:,'color']=Y['pred']\n",
    "    Y['embeddings2'].loc[:,'color']=Y['pred']\n",
    "    fig = px.scatter(Y['embeddings'], x=\"x\", y=\"y\", color=\"color\")\n",
    "    py.plot(fig, filename='figures/embeddings/embeddings.{}.pred.html'.format(epoch),auto_open=False)\n",
    "    fig = px.scatter(Y['embeddings2'], x=\"x\", y=\"y\", color=\"color\")\n",
    "    py.plot(fig, filename='figures/embeddings2/embeddings2.{}.pred.html'.format(epoch),auto_open=False)\n",
    "    print('Epoch {}: Val Loss {}, Margin Loss {}, Recon Loss {}, Val R2: {}, Val MAE: {}'.format(epoch,running_loss[0],running_loss[1],running_loss[2],r2_score(Y['true'],Y['pred']), mean_absolute_error(Y['true'],Y['pred'])))\n",
    "    #Y=pd.DataFrame([])\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim import Adam\n",
    "mlp=MLP(len(list(ma.beta)), [500,100], dropout_p=0., n_outputs=n_out_caps, binary=False, softmax=False)\n",
    "optimizer = Adam(mlp.parameters(),lr=1e-4,weight_decay=1e-3)\n",
    "scheduler=CosineAnnealingLR(optimizer, T_max=10, eta_min=0, last_epoch=-1)\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "mlp.train(True)\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss=0.\n",
    "    y_pred_all,y_true_all=[],[]\n",
    "    for i,batch in enumerate(dataloader):\n",
    "        x_orig=batch[0]\n",
    "        #print(x_orig.size())\n",
    "        #print(batch[-1])\n",
    "        #print(batch[-1].size())\n",
    "        y_true=torch.argmax(batch[-1],dim=1).long()\n",
    "        #print(y_true)\n",
    "        module_x = batch[1:-1]\n",
    "        y_pred=mlp(x_orig)\n",
    "        #print(y_pred.shape)\n",
    "        y_true_all.append(y_true.flatten().detach().numpy())\n",
    "        y_pred_all.append(torch.argmax(y_pred,dim=1).flatten().detach().numpy())\n",
    "        loss=loss_fn(y_pred,y_true)\n",
    "        train_loss=loss.item()#print(loss)\n",
    "        running_loss+=train_loss\n",
    "        #print(train_loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    y_true,y_pred=np.hstack(y_true_all),np.hstack(y_pred_all)\n",
    "    print(accuracy_score(y_true,y_pred))\n",
    "    running_loss/=(i+1)\n",
    "    print('train',running_loss)\n",
    "    scheduler.step()\n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
